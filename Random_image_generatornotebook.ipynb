{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6K5gkTQnWo9d"
      },
      "outputs": [],
      "source": [
        "!pip install opendatasets --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaurOhruW8Kd",
        "outputId": "4e35154b-23de-4cee-eeff-0329539262bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/greatgamedota/ffhq-face-data-set\n",
            "Downloading ffhq-face-data-set.zip to ./ffhq-face-data-set\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.97G/1.97G [00:18<00:00, 116MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import opendatasets as od\n",
        "\n",
        "dataset_url = 'https://www.kaggle.com/datasets/greatgamedota/ffhq-face-data-set'\n",
        "od.download(dataset_url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z0OHsbXFXuGp",
        "outputId": "35f8df17-a29d-4229-f687-0da4a21cee06"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Male Faces', 'Female Faces']\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "DATA_DIR = './male-and-female-faces-dataset/Male and Female face dataset'\n",
        "print(os.listdir(DATA_DIR))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fisub7Y7X4xv",
        "outputId": "e71ca71c-ce79-4023-ab78-b5968258c29e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['1 (1548).jpg', '1 (1915).jpg', '1 (1856).jpg', '1 (1994).jpg', '1 (1234).jpg', '1 (2579).jpg', '1 (2278).jpg', '1 (628).jpg', '1 (1572).jpg', '1 (1589).jpg']\n"
          ]
        }
      ],
      "source": [
        "print(os.listdir(DATA_DIR+'/Male Faces')[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9kxwcdwX8EW"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_Y8j1nUZ1n0",
        "outputId": "0f04d0a6-4ed3-43dc-b122-85ad002055bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = get_default_device()\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmy_IwWH8yGR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "def train_gan(dataloader, output_dir, num_epochs=100, latent_dim=100, lr=0.0002, beta1=0.5, device='cuda', checkpoint_path=None):\n",
        "    generator = Generator(latent_dim).to(device)\n",
        "    discriminator = Discriminator().to(device)\n",
        "\n",
        "    # Adjusted learning rates\n",
        "    g_optimizer = optim.Adam(generator.parameters(), lr=lr * 0.5, betas=(beta1, 0.999))\n",
        "    d_optimizer = optim.Adam(discriminator.parameters(), lr=lr * 0.2, betas=(beta1, 0.999))\n",
        "\n",
        "    # Learning rate schedulers to gradually decay learning rate\n",
        "    g_scheduler = StepLR(g_optimizer, step_size=100, gamma=0.8)\n",
        "    d_scheduler = StepLR(d_optimizer, step_size=100, gamma=0.8)\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "\n",
        "    # Initialize starting epoch\n",
        "    start_epoch = 0\n",
        "\n",
        "    # Load from checkpoint if provided\n",
        "    if checkpoint_path:\n",
        "        print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "        discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
        "        g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n",
        "        d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        print(f\"Resumed from epoch {start_epoch}\")\n",
        "\n",
        "    fixed_noise = torch.randn(64, latent_dim, device=device)\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        for i, (real_images, _) in enumerate(dataloader):\n",
        "            batch_size = real_images.size(0)\n",
        "            real_images = real_images.to(device)\n",
        "\n",
        "            # Label smoothing\n",
        "            real_labels = torch.ones(batch_size, device=device) * 0.9\n",
        "            fake_labels = torch.zeros(batch_size, device=device)\n",
        "\n",
        "            # Train Discriminator\n",
        "            d_optimizer.zero_grad()\n",
        "            output_real = discriminator(real_images)\n",
        "            d_loss_real = criterion(output_real, real_labels)\n",
        "\n",
        "            noise = torch.randn(batch_size, latent_dim, device=device)\n",
        "            fake_images = generator(noise)\n",
        "            output_fake = discriminator(fake_images.detach())\n",
        "            d_loss_fake = criterion(output_fake, fake_labels)\n",
        "\n",
        "            d_loss = (d_loss_real + d_loss_fake) / 2\n",
        "            d_loss.backward()\n",
        "            d_optimizer.step()\n",
        "\n",
        "            # Train Generator\n",
        "            g_optimizer.zero_grad()\n",
        "            output_fake = discriminator(fake_images)\n",
        "            g_loss = criterion(output_fake, real_labels)\n",
        "            g_loss.backward()\n",
        "            g_optimizer.step()\n",
        "\n",
        "            if i % 100 == 0:\n",
        "                print(f'Epoch [{epoch}/{num_epochs}] Batch [{i}/{len(dataloader)}] '\n",
        "                      f'D_loss: {d_loss.item():.4f} G_loss: {g_loss.item():.4f}')\n",
        "\n",
        "        # Adjust learning rates with the schedulers at the end of each epoch\n",
        "        g_scheduler.step()\n",
        "        d_scheduler.step()\n",
        "\n",
        "        if epoch % 5 == 0 or epoch == num_epochs - 1:\n",
        "            save_generated_images(generator, epoch, output_dir, fixed_noise=fixed_noise, device=device)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            torch.save({\n",
        "                'generator_state_dict': generator.state_dict(),\n",
        "                'discriminator_state_dict': discriminator.state_dict(),\n",
        "                'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
        "                'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
        "                'epoch': epoch,\n",
        "            }, os.path.join(output_dir, f'checkpoint_epoch_{epoch}.pt'))\n",
        "\n",
        "    return generator, discriminator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5KiOM6t9E7k",
        "outputId": "3e90c02e-7a0f-4ff5-a568-e9443da3ad2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 70000 images\n",
            "Using device: cuda\n",
            "Loading checkpoint from /content/checkpoint_epoch_450.pt...\n",
            "Resumed from epoch 451\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-6-3e244b254b1b>:25: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [451/1000] Batch [0/1093] D_loss: 0.1634 G_loss: 7.0972\n",
            "Epoch [451/1000] Batch [100/1093] D_loss: 0.1639 G_loss: 12.0856\n",
            "Epoch [451/1000] Batch [200/1093] D_loss: 0.1677 G_loss: 7.2193\n",
            "Epoch [451/1000] Batch [300/1093] D_loss: 0.1639 G_loss: 8.5507\n",
            "Epoch [451/1000] Batch [400/1093] D_loss: 0.1790 G_loss: 4.6491\n",
            "Epoch [451/1000] Batch [500/1093] D_loss: 0.1649 G_loss: 7.2039\n",
            "Epoch [451/1000] Batch [600/1093] D_loss: 0.1655 G_loss: 6.3064\n",
            "Epoch [451/1000] Batch [700/1093] D_loss: 0.1646 G_loss: 6.7214\n",
            "Epoch [451/1000] Batch [800/1093] D_loss: 0.1645 G_loss: 8.4637\n",
            "Epoch [451/1000] Batch [900/1093] D_loss: 0.1638 G_loss: 6.6527\n",
            "Epoch [451/1000] Batch [1000/1093] D_loss: 0.1642 G_loss: 7.6956\n",
            "Epoch [452/1000] Batch [0/1093] D_loss: 0.2187 G_loss: 4.1996\n",
            "Epoch [452/1000] Batch [100/1093] D_loss: 0.1667 G_loss: 7.3881\n",
            "Epoch [452/1000] Batch [200/1093] D_loss: 0.1642 G_loss: 6.8492\n",
            "Epoch [452/1000] Batch [300/1093] D_loss: 0.1654 G_loss: 6.3942\n",
            "Epoch [452/1000] Batch [400/1093] D_loss: 0.1660 G_loss: 5.9431\n",
            "Epoch [452/1000] Batch [500/1093] D_loss: 0.1685 G_loss: 5.5115\n",
            "Epoch [452/1000] Batch [600/1093] D_loss: 0.1637 G_loss: 6.2786\n",
            "Epoch [452/1000] Batch [700/1093] D_loss: 0.1691 G_loss: 6.1688\n",
            "Epoch [452/1000] Batch [800/1093] D_loss: 0.1653 G_loss: 7.6813\n",
            "Epoch [452/1000] Batch [900/1093] D_loss: 0.1643 G_loss: 8.0015\n",
            "Epoch [452/1000] Batch [1000/1093] D_loss: 0.1697 G_loss: 5.0604\n",
            "Epoch [453/1000] Batch [0/1093] D_loss: 0.1641 G_loss: 6.1804\n",
            "Epoch [453/1000] Batch [100/1093] D_loss: 0.1794 G_loss: 5.6600\n",
            "Epoch [453/1000] Batch [200/1093] D_loss: 0.1654 G_loss: 6.2944\n",
            "Epoch [453/1000] Batch [300/1093] D_loss: 0.1671 G_loss: 9.5321\n",
            "Epoch [453/1000] Batch [400/1093] D_loss: 0.1657 G_loss: 6.9291\n",
            "Epoch [453/1000] Batch [500/1093] D_loss: 0.1647 G_loss: 6.2507\n",
            "Epoch [453/1000] Batch [600/1093] D_loss: 0.1686 G_loss: 6.4312\n",
            "Epoch [453/1000] Batch [700/1093] D_loss: 0.1662 G_loss: 7.9177\n",
            "Epoch [453/1000] Batch [800/1093] D_loss: 0.1639 G_loss: 6.4174\n",
            "Epoch [453/1000] Batch [900/1093] D_loss: 0.1634 G_loss: 8.3755\n",
            "Epoch [453/1000] Batch [1000/1093] D_loss: 0.1647 G_loss: 7.6485\n",
            "Epoch [454/1000] Batch [0/1093] D_loss: 0.1640 G_loss: 7.4885\n",
            "Epoch [454/1000] Batch [100/1093] D_loss: 0.1721 G_loss: 6.3087\n",
            "Epoch [454/1000] Batch [200/1093] D_loss: 0.1638 G_loss: 9.6160\n",
            "Epoch [454/1000] Batch [300/1093] D_loss: 0.1643 G_loss: 7.0050\n",
            "Epoch [454/1000] Batch [400/1093] D_loss: 0.1672 G_loss: 5.7319\n",
            "Epoch [454/1000] Batch [500/1093] D_loss: 0.1686 G_loss: 7.4511\n",
            "Epoch [454/1000] Batch [600/1093] D_loss: 0.1675 G_loss: 6.5848\n",
            "Epoch [454/1000] Batch [700/1093] D_loss: 0.1689 G_loss: 7.8608\n",
            "Epoch [454/1000] Batch [800/1093] D_loss: 0.1658 G_loss: 5.9440\n",
            "Epoch [454/1000] Batch [900/1093] D_loss: 0.1647 G_loss: 7.7339\n",
            "Epoch [454/1000] Batch [1000/1093] D_loss: 0.1680 G_loss: 6.6364\n",
            "Epoch [455/1000] Batch [0/1093] D_loss: 0.1637 G_loss: 7.0817\n",
            "Epoch [455/1000] Batch [100/1093] D_loss: 0.1736 G_loss: 4.9690\n",
            "Epoch [455/1000] Batch [200/1093] D_loss: 0.1657 G_loss: 7.0404\n",
            "Epoch [455/1000] Batch [300/1093] D_loss: 0.1671 G_loss: 6.0272\n",
            "Epoch [455/1000] Batch [400/1093] D_loss: 0.1646 G_loss: 9.4844\n",
            "Epoch [455/1000] Batch [500/1093] D_loss: 0.1659 G_loss: 7.9047\n",
            "Epoch [455/1000] Batch [600/1093] D_loss: 0.1708 G_loss: 6.4517\n",
            "Epoch [455/1000] Batch [700/1093] D_loss: 0.1668 G_loss: 6.0997\n",
            "Epoch [455/1000] Batch [800/1093] D_loss: 0.1678 G_loss: 7.5788\n",
            "Epoch [455/1000] Batch [900/1093] D_loss: 0.1647 G_loss: 6.4777\n",
            "Epoch [455/1000] Batch [1000/1093] D_loss: 0.1635 G_loss: 7.7126\n",
            "Epoch [456/1000] Batch [0/1093] D_loss: 0.1637 G_loss: 8.4095\n",
            "Epoch [456/1000] Batch [100/1093] D_loss: 0.1641 G_loss: 6.6213\n",
            "Epoch [456/1000] Batch [200/1093] D_loss: 0.1644 G_loss: 7.9049\n",
            "Epoch [456/1000] Batch [300/1093] D_loss: 0.1665 G_loss: 6.5670\n",
            "Epoch [456/1000] Batch [400/1093] D_loss: 0.1646 G_loss: 5.8959\n",
            "Epoch [456/1000] Batch [500/1093] D_loss: 0.1982 G_loss: 6.1598\n",
            "Epoch [456/1000] Batch [600/1093] D_loss: 0.1678 G_loss: 5.7521\n",
            "Epoch [456/1000] Batch [700/1093] D_loss: 0.1681 G_loss: 6.1757\n",
            "Epoch [456/1000] Batch [800/1093] D_loss: 0.1641 G_loss: 6.2215\n",
            "Epoch [456/1000] Batch [900/1093] D_loss: 0.1640 G_loss: 8.5760\n",
            "Epoch [456/1000] Batch [1000/1093] D_loss: 0.1636 G_loss: 7.6206\n",
            "Epoch [457/1000] Batch [0/1093] D_loss: 0.1707 G_loss: 9.9003\n",
            "Epoch [457/1000] Batch [100/1093] D_loss: 0.1639 G_loss: 6.8414\n",
            "Epoch [457/1000] Batch [200/1093] D_loss: 0.1641 G_loss: 7.0111\n",
            "Epoch [457/1000] Batch [300/1093] D_loss: 0.1632 G_loss: 8.0154\n",
            "Epoch [457/1000] Batch [400/1093] D_loss: 0.1665 G_loss: 6.4704\n",
            "Epoch [457/1000] Batch [500/1093] D_loss: 0.1634 G_loss: 7.3494\n",
            "Epoch [457/1000] Batch [600/1093] D_loss: 0.1632 G_loss: 9.4637\n",
            "Epoch [457/1000] Batch [700/1093] D_loss: 0.1658 G_loss: 7.4414\n",
            "Epoch [457/1000] Batch [800/1093] D_loss: 0.1634 G_loss: 6.7667\n",
            "Epoch [457/1000] Batch [900/1093] D_loss: 0.1634 G_loss: 10.1553\n",
            "Epoch [457/1000] Batch [1000/1093] D_loss: 0.1753 G_loss: 5.3350\n",
            "Epoch [458/1000] Batch [0/1093] D_loss: 0.1635 G_loss: 7.5638\n",
            "Epoch [458/1000] Batch [100/1093] D_loss: 0.1631 G_loss: 7.9609\n",
            "Epoch [458/1000] Batch [200/1093] D_loss: 0.1646 G_loss: 8.0616\n",
            "Epoch [458/1000] Batch [300/1093] D_loss: 0.1635 G_loss: 7.0365\n",
            "Epoch [458/1000] Batch [400/1093] D_loss: 0.1636 G_loss: 6.7035\n",
            "Epoch [458/1000] Batch [500/1093] D_loss: 0.1639 G_loss: 6.9635\n",
            "Epoch [458/1000] Batch [600/1093] D_loss: 0.1641 G_loss: 8.3427\n",
            "Epoch [458/1000] Batch [700/1093] D_loss: 0.1636 G_loss: 6.8860\n",
            "Epoch [458/1000] Batch [800/1093] D_loss: 0.1641 G_loss: 6.1999\n",
            "Epoch [458/1000] Batch [900/1093] D_loss: 0.1640 G_loss: 7.8808\n",
            "Epoch [458/1000] Batch [1000/1093] D_loss: 0.1660 G_loss: 6.9723\n",
            "Epoch [459/1000] Batch [0/1093] D_loss: 0.1635 G_loss: 6.9125\n",
            "Epoch [459/1000] Batch [100/1093] D_loss: 0.1637 G_loss: 7.6721\n",
            "Epoch [459/1000] Batch [200/1093] D_loss: 0.1719 G_loss: 8.5954\n",
            "Epoch [459/1000] Batch [300/1093] D_loss: 0.1732 G_loss: 7.4946\n",
            "Epoch [459/1000] Batch [400/1093] D_loss: 0.1672 G_loss: 6.2849\n",
            "Epoch [459/1000] Batch [500/1093] D_loss: 0.1675 G_loss: 6.3293\n",
            "Epoch [459/1000] Batch [600/1093] D_loss: 0.1679 G_loss: 6.3211\n",
            "Epoch [459/1000] Batch [700/1093] D_loss: 0.1647 G_loss: 6.9454\n",
            "Epoch [459/1000] Batch [800/1093] D_loss: 0.1665 G_loss: 5.9980\n",
            "Epoch [459/1000] Batch [900/1093] D_loss: 0.1648 G_loss: 6.1645\n",
            "Epoch [459/1000] Batch [1000/1093] D_loss: 0.1647 G_loss: 6.4277\n",
            "Epoch [460/1000] Batch [0/1093] D_loss: 0.1647 G_loss: 6.4873\n",
            "Epoch [460/1000] Batch [100/1093] D_loss: 0.1638 G_loss: 6.2336\n",
            "Epoch [460/1000] Batch [200/1093] D_loss: 0.1641 G_loss: 7.6303\n",
            "Epoch [460/1000] Batch [300/1093] D_loss: 0.1643 G_loss: 7.6397\n",
            "Epoch [460/1000] Batch [400/1093] D_loss: 0.1631 G_loss: 7.6397\n",
            "Epoch [460/1000] Batch [500/1093] D_loss: 0.1663 G_loss: 6.6777\n",
            "Epoch [460/1000] Batch [600/1093] D_loss: 0.1649 G_loss: 7.3960\n",
            "Epoch [460/1000] Batch [700/1093] D_loss: 0.1639 G_loss: 7.6371\n",
            "Epoch [460/1000] Batch [800/1093] D_loss: 0.1664 G_loss: 8.1072\n",
            "Epoch [460/1000] Batch [900/1093] D_loss: 0.1727 G_loss: 10.2693\n",
            "Epoch [460/1000] Batch [1000/1093] D_loss: 0.1639 G_loss: 7.1826\n",
            "Epoch [461/1000] Batch [0/1093] D_loss: 0.1644 G_loss: 5.8507\n",
            "Epoch [461/1000] Batch [100/1093] D_loss: 0.1648 G_loss: 8.1393\n",
            "Epoch [461/1000] Batch [200/1093] D_loss: 0.1688 G_loss: 7.1889\n",
            "Epoch [461/1000] Batch [300/1093] D_loss: 0.1638 G_loss: 6.8012\n",
            "Epoch [461/1000] Batch [400/1093] D_loss: 0.1647 G_loss: 7.4440\n",
            "Epoch [461/1000] Batch [500/1093] D_loss: 0.1644 G_loss: 8.4506\n",
            "Epoch [461/1000] Batch [600/1093] D_loss: 0.1669 G_loss: 6.0243\n",
            "Epoch [461/1000] Batch [700/1093] D_loss: 0.1643 G_loss: 8.6958\n",
            "Epoch [461/1000] Batch [800/1093] D_loss: 0.1694 G_loss: 7.4154\n",
            "Epoch [461/1000] Batch [900/1093] D_loss: 0.1635 G_loss: 7.3971\n",
            "Epoch [461/1000] Batch [1000/1093] D_loss: 0.1703 G_loss: 7.3356\n",
            "Epoch [462/1000] Batch [0/1093] D_loss: 0.1635 G_loss: 7.7003\n",
            "Epoch [462/1000] Batch [100/1093] D_loss: 0.1651 G_loss: 7.1399\n",
            "Epoch [462/1000] Batch [200/1093] D_loss: 0.1641 G_loss: 5.8817\n",
            "Epoch [462/1000] Batch [300/1093] D_loss: 0.1663 G_loss: 7.1091\n",
            "Epoch [462/1000] Batch [400/1093] D_loss: 0.1651 G_loss: 8.7012\n",
            "Epoch [462/1000] Batch [500/1093] D_loss: 0.1647 G_loss: 7.7710\n",
            "Epoch [462/1000] Batch [600/1093] D_loss: 0.1634 G_loss: 8.0548\n",
            "Epoch [462/1000] Batch [700/1093] D_loss: 0.1820 G_loss: 8.6527\n",
            "Epoch [462/1000] Batch [800/1093] D_loss: 0.1650 G_loss: 7.4961\n",
            "Epoch [462/1000] Batch [900/1093] D_loss: 0.1698 G_loss: 8.4470\n",
            "Epoch [462/1000] Batch [1000/1093] D_loss: 0.1671 G_loss: 5.4080\n",
            "Epoch [463/1000] Batch [0/1093] D_loss: 0.1648 G_loss: 7.3271\n",
            "Epoch [463/1000] Batch [100/1093] D_loss: 0.1644 G_loss: 8.1919\n",
            "Epoch [463/1000] Batch [200/1093] D_loss: 0.1779 G_loss: 6.7484\n",
            "Epoch [463/1000] Batch [300/1093] D_loss: 0.1696 G_loss: 6.1103\n",
            "Epoch [463/1000] Batch [400/1093] D_loss: 0.1680 G_loss: 5.7325\n",
            "Epoch [463/1000] Batch [500/1093] D_loss: 0.1651 G_loss: 6.3652\n",
            "Epoch [463/1000] Batch [600/1093] D_loss: 0.1714 G_loss: 7.1444\n",
            "Epoch [463/1000] Batch [700/1093] D_loss: 0.1669 G_loss: 5.3812\n",
            "Epoch [463/1000] Batch [800/1093] D_loss: 0.1633 G_loss: 7.1330\n",
            "Epoch [463/1000] Batch [900/1093] D_loss: 0.1632 G_loss: 8.4871\n",
            "Epoch [463/1000] Batch [1000/1093] D_loss: 0.1644 G_loss: 6.6693\n",
            "Epoch [464/1000] Batch [0/1093] D_loss: 0.1643 G_loss: 6.2250\n",
            "Epoch [464/1000] Batch [100/1093] D_loss: 0.1647 G_loss: 7.5234\n",
            "Epoch [464/1000] Batch [200/1093] D_loss: 0.1649 G_loss: 6.9634\n",
            "Epoch [464/1000] Batch [300/1093] D_loss: 0.1635 G_loss: 7.6530\n",
            "Epoch [464/1000] Batch [400/1093] D_loss: 0.1636 G_loss: 6.7793\n",
            "Epoch [464/1000] Batch [500/1093] D_loss: 0.1761 G_loss: 6.6382\n",
            "Epoch [464/1000] Batch [600/1093] D_loss: 0.1656 G_loss: 7.0027\n",
            "Epoch [464/1000] Batch [700/1093] D_loss: 0.1633 G_loss: 7.0809\n",
            "Epoch [464/1000] Batch [800/1093] D_loss: 0.1641 G_loss: 9.6849\n",
            "Epoch [464/1000] Batch [900/1093] D_loss: 0.1632 G_loss: 7.5112\n",
            "Epoch [464/1000] Batch [1000/1093] D_loss: 0.1636 G_loss: 7.3291\n",
            "Epoch [465/1000] Batch [0/1093] D_loss: 0.1634 G_loss: 9.1373\n",
            "Epoch [465/1000] Batch [100/1093] D_loss: 0.1631 G_loss: 7.5822\n",
            "Epoch [465/1000] Batch [200/1093] D_loss: 0.1631 G_loss: 8.8903\n",
            "Epoch [465/1000] Batch [300/1093] D_loss: 0.1691 G_loss: 7.1794\n",
            "Epoch [465/1000] Batch [400/1093] D_loss: 0.1635 G_loss: 6.5679\n",
            "Epoch [465/1000] Batch [500/1093] D_loss: 0.1632 G_loss: 12.0197\n",
            "Epoch [465/1000] Batch [600/1093] D_loss: 0.1636 G_loss: 7.2954\n",
            "Epoch [465/1000] Batch [700/1093] D_loss: 0.1632 G_loss: 7.6771\n",
            "Epoch [465/1000] Batch [800/1093] D_loss: 0.1835 G_loss: 6.4487\n",
            "Epoch [465/1000] Batch [900/1093] D_loss: 0.1715 G_loss: 8.7864\n",
            "Epoch [465/1000] Batch [1000/1093] D_loss: 0.1642 G_loss: 6.6663\n",
            "Epoch [466/1000] Batch [0/1093] D_loss: 0.1635 G_loss: 7.7666\n",
            "Epoch [466/1000] Batch [100/1093] D_loss: 0.1642 G_loss: 7.1810\n",
            "Epoch [466/1000] Batch [200/1093] D_loss: 0.1713 G_loss: 4.7763\n",
            "Epoch [466/1000] Batch [300/1093] D_loss: 0.1651 G_loss: 7.3682\n",
            "Epoch [466/1000] Batch [400/1093] D_loss: 0.1638 G_loss: 7.4883\n",
            "Epoch [466/1000] Batch [500/1093] D_loss: 0.1680 G_loss: 8.2979\n",
            "Epoch [466/1000] Batch [600/1093] D_loss: 0.1638 G_loss: 6.7490\n",
            "Epoch [466/1000] Batch [700/1093] D_loss: 0.1646 G_loss: 7.1865\n",
            "Epoch [466/1000] Batch [800/1093] D_loss: 0.1645 G_loss: 6.5801\n",
            "Epoch [466/1000] Batch [900/1093] D_loss: 0.1647 G_loss: 8.7106\n",
            "Epoch [466/1000] Batch [1000/1093] D_loss: 0.1640 G_loss: 7.4754\n",
            "Epoch [467/1000] Batch [0/1093] D_loss: 0.1657 G_loss: 6.8997\n",
            "Epoch [467/1000] Batch [100/1093] D_loss: 0.1631 G_loss: 8.1207\n",
            "Epoch [467/1000] Batch [200/1093] D_loss: 0.1635 G_loss: 7.4660\n",
            "Epoch [467/1000] Batch [300/1093] D_loss: 0.1632 G_loss: 7.9548\n",
            "Epoch [467/1000] Batch [400/1093] D_loss: 0.1638 G_loss: 7.0079\n",
            "Epoch [467/1000] Batch [500/1093] D_loss: 0.1693 G_loss: 7.2518\n",
            "Epoch [467/1000] Batch [600/1093] D_loss: 0.1949 G_loss: 9.1253\n",
            "Epoch [467/1000] Batch [700/1093] D_loss: 0.1642 G_loss: 11.7124\n",
            "Epoch [467/1000] Batch [800/1093] D_loss: 0.1642 G_loss: 6.9862\n",
            "Epoch [467/1000] Batch [900/1093] D_loss: 0.1644 G_loss: 8.2710\n",
            "Epoch [467/1000] Batch [1000/1093] D_loss: 0.1648 G_loss: 6.4835\n",
            "Epoch [468/1000] Batch [0/1093] D_loss: 0.1648 G_loss: 5.7338\n",
            "Epoch [468/1000] Batch [100/1093] D_loss: 0.1802 G_loss: 7.7997\n",
            "Epoch [468/1000] Batch [200/1093] D_loss: 0.1689 G_loss: 9.6934\n",
            "Epoch [468/1000] Batch [300/1093] D_loss: 0.1634 G_loss: 7.2318\n",
            "Epoch [468/1000] Batch [400/1093] D_loss: 0.1649 G_loss: 7.6102\n",
            "Epoch [468/1000] Batch [500/1093] D_loss: 0.1649 G_loss: 6.9026\n",
            "Epoch [468/1000] Batch [600/1093] D_loss: 0.1643 G_loss: 8.4315\n",
            "Epoch [468/1000] Batch [700/1093] D_loss: 0.1679 G_loss: 7.4522\n",
            "Epoch [468/1000] Batch [800/1093] D_loss: 0.1647 G_loss: 6.8726\n",
            "Epoch [468/1000] Batch [900/1093] D_loss: 0.1640 G_loss: 6.8080\n",
            "Epoch [468/1000] Batch [1000/1093] D_loss: 0.1637 G_loss: 7.9554\n",
            "Epoch [469/1000] Batch [0/1093] D_loss: 0.1638 G_loss: 6.9592\n",
            "Epoch [469/1000] Batch [100/1093] D_loss: 0.1639 G_loss: 6.4066\n",
            "Epoch [469/1000] Batch [200/1093] D_loss: 0.1639 G_loss: 7.2847\n",
            "Epoch [469/1000] Batch [300/1093] D_loss: 0.1649 G_loss: 5.9924\n",
            "Epoch [469/1000] Batch [400/1093] D_loss: 0.1643 G_loss: 6.8609\n",
            "Epoch [469/1000] Batch [500/1093] D_loss: 0.1638 G_loss: 7.2255\n",
            "Epoch [469/1000] Batch [600/1093] D_loss: 0.1642 G_loss: 6.4120\n",
            "Epoch [469/1000] Batch [700/1093] D_loss: 0.1644 G_loss: 9.7014\n",
            "Epoch [469/1000] Batch [800/1093] D_loss: 0.1646 G_loss: 7.2599\n",
            "Epoch [469/1000] Batch [900/1093] D_loss: 0.1701 G_loss: 6.8326\n",
            "Epoch [469/1000] Batch [1000/1093] D_loss: 0.1674 G_loss: 9.9059\n",
            "Epoch [470/1000] Batch [0/1093] D_loss: 0.1632 G_loss: 9.1273\n",
            "Epoch [470/1000] Batch [100/1093] D_loss: 0.1644 G_loss: 6.5574\n",
            "Epoch [470/1000] Batch [200/1093] D_loss: 0.1681 G_loss: 9.7784\n",
            "Epoch [470/1000] Batch [300/1093] D_loss: 0.2851 G_loss: 17.7514\n",
            "Epoch [470/1000] Batch [400/1093] D_loss: 0.1648 G_loss: 5.9040\n",
            "Epoch [470/1000] Batch [500/1093] D_loss: 0.1663 G_loss: 7.6115\n",
            "Epoch [470/1000] Batch [600/1093] D_loss: 0.1718 G_loss: 6.0969\n",
            "Epoch [470/1000] Batch [700/1093] D_loss: 0.1637 G_loss: 7.2105\n",
            "Epoch [470/1000] Batch [800/1093] D_loss: 0.1651 G_loss: 5.9264\n",
            "Epoch [470/1000] Batch [900/1093] D_loss: 0.1657 G_loss: 6.4902\n",
            "Epoch [470/1000] Batch [1000/1093] D_loss: 0.1657 G_loss: 5.6648\n",
            "Epoch [471/1000] Batch [0/1093] D_loss: 0.1640 G_loss: 6.9073\n",
            "Epoch [471/1000] Batch [100/1093] D_loss: 0.1693 G_loss: 7.8242\n",
            "Epoch [471/1000] Batch [200/1093] D_loss: 0.1770 G_loss: 7.1263\n",
            "Epoch [471/1000] Batch [300/1093] D_loss: 0.1655 G_loss: 6.4717\n",
            "Epoch [471/1000] Batch [400/1093] D_loss: 0.1634 G_loss: 7.1594\n",
            "Epoch [471/1000] Batch [500/1093] D_loss: 0.1639 G_loss: 6.2657\n",
            "Epoch [471/1000] Batch [600/1093] D_loss: 0.1661 G_loss: 6.0891\n",
            "Epoch [471/1000] Batch [700/1093] D_loss: 0.1736 G_loss: 4.4229\n",
            "Epoch [471/1000] Batch [800/1093] D_loss: 0.1638 G_loss: 7.9950\n",
            "Epoch [471/1000] Batch [900/1093] D_loss: 0.1774 G_loss: 4.9777\n",
            "Epoch [471/1000] Batch [1000/1093] D_loss: 0.1643 G_loss: 6.8090\n",
            "Epoch [472/1000] Batch [0/1093] D_loss: 0.1690 G_loss: 9.4687\n",
            "Epoch [472/1000] Batch [100/1093] D_loss: 0.1635 G_loss: 7.8362\n",
            "Epoch [472/1000] Batch [200/1093] D_loss: 0.1650 G_loss: 6.5628\n",
            "Epoch [472/1000] Batch [300/1093] D_loss: 0.1661 G_loss: 8.4892\n",
            "Epoch [472/1000] Batch [400/1093] D_loss: 0.1647 G_loss: 6.9692\n",
            "Epoch [472/1000] Batch [500/1093] D_loss: 0.1638 G_loss: 7.0598\n",
            "Epoch [472/1000] Batch [600/1093] D_loss: 0.1632 G_loss: 8.0562\n",
            "Epoch [472/1000] Batch [700/1093] D_loss: 0.1683 G_loss: 7.9084\n",
            "Epoch [472/1000] Batch [800/1093] D_loss: 0.1785 G_loss: 8.1861\n",
            "Epoch [472/1000] Batch [900/1093] D_loss: 0.1690 G_loss: 6.3042\n",
            "Epoch [472/1000] Batch [1000/1093] D_loss: 0.1637 G_loss: 8.6591\n",
            "Epoch [473/1000] Batch [0/1093] D_loss: 0.1634 G_loss: 8.2282\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Update these paths as needed\n",
        "data_path = '/content/ffhq-face-data-set/thumbnails128x128'  # Path to the folder containing images\n",
        "output_dir = 'generated_images'    # Directory to save generated images and model checkpoints\n",
        "\n",
        "# Set parameters\n",
        "image_size = 64         # Image size expected by the model\n",
        "batch_size = 64         # Number of images in each batch\n",
        "num_epochs = 1000        # Total number of epochs (try starting with 100 and adjust as needed)\n",
        "latent_dim = 100        # Dimensionality of the latent space\n",
        "lr = 0.0002             # Base learning rate\n",
        "checkpoint_path = '/content/checkpoint_epoch_450.pt'  # Optional: Path to a previous checkpoint to resume training\n",
        "\n",
        "# Define the data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((image_size, image_size)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "# Create the dataset and dataloader\n",
        "dataset = FaceDataset(data_path, transform=transform)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
        "\n",
        "# Set device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Start training\n",
        "generator, discriminator = train_gan(\n",
        "    dataloader=dataloader,\n",
        "    output_dir=output_dir,\n",
        "    num_epochs=num_epochs,\n",
        "    latent_dim=latent_dim,\n",
        "    lr=lr,\n",
        "    device=device,\n",
        "    checkpoint_path=checkpoint_path\n",
        ")\n",
        "\n",
        "print(\"Training completed.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}